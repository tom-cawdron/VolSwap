"""
GRU Regime Classifier (PyTorch).

Trains a 2-layer GRU on sliding windows of volatility features,
predicting the binary regime label generated by the HMM baseline.

Usage:
    python -m src.gru           # from ml/
    python ml/src/gru.py        # from repo root
"""

import os
from pathlib import Path

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import classification_report, roc_auc_score

try:
    from src.features import FEATURE_COLS
except ImportError:
    from features import FEATURE_COLS


# ---------------------------------------------------------------------------
# Paths & Config
# ---------------------------------------------------------------------------
PROJECT_ROOT = Path(__file__).resolve().parents[1]
MODEL_DIR = PROJECT_ROOT / "models"
DATA_DIR = PROJECT_ROOT / "data"

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Hyperparameters
SEQ_LEN = 48           # sliding window: 48 hours (2 days)
HIDDEN_DIM = 64
NUM_LAYERS = 2
DROPOUT = 0.2
BATCH_SIZE = 64
EPOCHS = 30
LR = 1e-3
TRAIN_RATIO = 0.8      # temporal split — no shuffle!


# ---------------------------------------------------------------------------
# Dataset
# ---------------------------------------------------------------------------

class RegimeDataset(Dataset):
    """Sliding-window dataset for regime classification."""

    def __init__(self, features: np.ndarray, labels: np.ndarray, seq_len: int = SEQ_LEN):
        self.features = features
        self.labels = labels
        self.seq_len = seq_len

    def __len__(self) -> int:
        return len(self.features) - self.seq_len

    def __getitem__(self, idx: int):
        x = self.features[idx : idx + self.seq_len]
        y = self.labels[idx + self.seq_len - 1]  # label at end of window
        return (
            torch.tensor(x, dtype=torch.float32),
            torch.tensor(y, dtype=torch.long),
        )


# ---------------------------------------------------------------------------
# Model
# ---------------------------------------------------------------------------

class RegimeGRU(nn.Module):
    """
    2-layer GRU with softmax output for binary regime classification.

    Forward returns probability vector [P(LOW_VOL), P(HIGH_VOL)].
    """

    def __init__(
        self,
        input_dim: int,
        hidden_dim: int = HIDDEN_DIM,
        num_layers: int = NUM_LAYERS,
        dropout: float = DROPOUT,
    ):
        super().__init__()
        self.gru = nn.GRU(
            input_dim, hidden_dim, num_layers,
            batch_first=True, dropout=dropout,
        )
        self.fc = nn.Linear(hidden_dim, 2)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: (batch, seq_len, input_dim)
        Returns:
            probs: (batch, 2) — softmax probabilities
        """
        out, _ = self.gru(x)
        logits = self.fc(out[:, -1, :])  # last timestep
        return torch.softmax(logits, dim=-1)


# ---------------------------------------------------------------------------
# Training utilities
# ---------------------------------------------------------------------------

def load_labelled_data() -> pd.DataFrame:
    """Load the HMM-labelled feature CSV produced by hmm.py."""
    path = DATA_DIR / "labelled_features.csv"
    if not path.exists():
        raise FileNotFoundError(
            f"{path} not found. Run `python src/hmm.py` first to generate labels."
        )
    df = pd.read_csv(path, index_col=0, parse_dates=True)
    return df


def create_dataloaders(df: pd.DataFrame):
    """Temporal train/val split and DataLoader creation."""
    features = df[FEATURE_COLS].values
    labels = df["regime_label"].values

    # Normalise features (fit on train, apply to both)
    split = int(len(features) * TRAIN_RATIO)
    mean = features[:split].mean(axis=0)
    std = features[:split].std(axis=0) + 1e-8
    features = (features - mean) / std

    train_ds = RegimeDataset(features[:split], labels[:split])
    val_ds = RegimeDataset(features[split:], labels[split:])

    train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=False)
    val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)

    return train_dl, val_dl, mean, std


def train_one_epoch(model, loader, criterion, optimiser):
    model.train()
    total_loss, correct, total = 0.0, 0, 0
    for x, y in loader:
        x, y = x.to(DEVICE), y.to(DEVICE)
        probs = model(x)
        loss = criterion(probs, y)
        optimiser.zero_grad()
        loss.backward()
        optimiser.step()
        total_loss += loss.item() * len(y)
        correct += (probs.argmax(dim=1) == y).sum().item()
        total += len(y)
    return total_loss / total, correct / total


@torch.no_grad()
def evaluate(model, loader, criterion):
    model.eval()
    total_loss, correct, total = 0.0, 0, 0
    all_probs, all_labels = [], []
    for x, y in loader:
        x, y = x.to(DEVICE), y.to(DEVICE)
        probs = model(x)
        loss = criterion(probs, y)
        total_loss += loss.item() * len(y)
        correct += (probs.argmax(dim=1) == y).sum().item()
        total += len(y)
        all_probs.append(probs[:, 1].cpu().numpy())
        all_labels.append(y.cpu().numpy())
    all_probs = np.concatenate(all_probs)
    all_labels = np.concatenate(all_labels)
    auc = roc_auc_score(all_labels, all_probs) if len(np.unique(all_labels)) > 1 else 0.0
    return total_loss / total, correct / total, auc


# ---------------------------------------------------------------------------
# Main
# ---------------------------------------------------------------------------

def main() -> None:
    print("=" * 60)
    print("GRU Regime Classifier — Training")
    print("=" * 60)

    df = load_labelled_data()
    print(f"Loaded {len(df)} labelled samples ({FEATURE_COLS})")

    train_dl, val_dl, feat_mean, feat_std = create_dataloaders(df)
    print(f"Train batches: {len(train_dl)}, Val batches: {len(val_dl)}")

    model = RegimeGRU(input_dim=len(FEATURE_COLS)).to(DEVICE)
    criterion = nn.CrossEntropyLoss()
    optimiser = torch.optim.Adam(model.parameters(), lr=LR)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimiser, mode="min", factor=0.5, patience=3,
    )

    best_auc = 0.0
    best_val_loss = float("inf")
    for epoch in range(1, EPOCHS + 1):
        train_loss, train_acc = train_one_epoch(model, train_dl, criterion, optimiser)
        val_loss, val_acc, val_auc = evaluate(model, val_dl, criterion)
        scheduler.step(val_loss)

        print(
            f"Epoch {epoch:02d}/{EPOCHS} | "
            f"Train Loss {train_loss:.4f} Acc {train_acc:.3f} | "
            f"Val Loss {val_loss:.4f} Acc {val_acc:.3f} AUC {val_auc:.3f}"
        )

        # Save best model — prefer AUC when available, fall back to val loss
        improved = False
        if val_auc > best_auc:
            best_auc = val_auc
            improved = True
        if val_auc == 0.0 and val_loss < best_val_loss:
            best_val_loss = val_loss
            improved = True

        if improved:
            MODEL_DIR.mkdir(exist_ok=True)
            torch.save(model.state_dict(), MODEL_DIR / "regime_gru.pt")

    print(f"\nBest Val AUC: {best_auc:.4f} | Best Val Loss: {best_val_loss:.4f}")

    # Save TorchScript for inference
    model.load_state_dict(torch.load(MODEL_DIR / "regime_gru.pt", weights_only=True))
    model.eval()
    scripted = torch.jit.script(model)
    torch.jit.save(scripted, MODEL_DIR / "regime_gru_scripted.pt")
    print(f"Scripted model saved → {MODEL_DIR / 'regime_gru_scripted.pt'}")

    # Save normalisation params
    np.savez(
        MODEL_DIR / "feature_norm.npz",
        mean=feat_mean,
        std=feat_std,
    )
    print(f"Feature normalisation saved → {MODEL_DIR / 'feature_norm.npz'}")

    # Final classification report on validation set
    model.to(DEVICE)
    all_preds, all_labels = [], []
    for x, y in val_dl:
        x = x.to(DEVICE)
        probs = model(x)
        all_preds.extend(probs.argmax(dim=1).cpu().numpy())
        all_labels.extend(y.numpy())
    print("\nValidation Classification Report:")
    unique_labels = sorted(set(all_labels))
    names = [["LOW_VOL", "HIGH_VOL"][i] for i in unique_labels]
    print(classification_report(all_labels, all_preds, labels=unique_labels, target_names=names))

    print("Done ✓")


if __name__ == "__main__":
    main()
